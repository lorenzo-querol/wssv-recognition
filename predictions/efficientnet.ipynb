{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lorenzoquerol/miniconda3/envs/wssv-recognition/lib/python3.9/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1023 files belonging to 2 classes.\n",
      "Found 49 files belonging to 2 classes.\n",
      "Found 13 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "CONFIG = dict(\n",
    "    epochs=100,\n",
    "    learning_rate=1e-4,\n",
    "    batch_size=4,\n",
    "    img_shape=(224, 224),\n",
    "    input_shape=(224, 224, 3),\n",
    "    num_classes=2,\n",
    "    dropout_rate=0.6,\n",
    "    es_patience=10,\n",
    "    seed_value=42,\n",
    ")\n",
    "TRAIN_DIR = \"dataset-augmented/train\"\n",
    "VAL_DIR = \"dataset-augmented/val\"\n",
    "TEST_DIR = \"dataset-augmented/test\"\n",
    "CLASS_NAMES = [\"healthy\", \"wssv\"]\n",
    "\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    seed=CONFIG[\"seed_value\"],\n",
    "    image_size=CONFIG[\"img_shape\"],\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    label_mode=\"categorical\",\n",
    ").prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "valid_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    VAL_DIR,\n",
    "    seed=CONFIG[\"seed_value\"],\n",
    "    image_size=CONFIG[\"img_shape\"],\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    label_mode=\"categorical\",\n",
    ").prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    TEST_DIR,\n",
    "    seed=CONFIG[\"seed_value\"],\n",
    "    image_size=CONFIG[\"img_shape\"],\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    label_mode=\"categorical\",\n",
    ").prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    base_model = tf.keras.applications.EfficientNetV2B0(\n",
    "        input_shape=CONFIG[\"input_shape\"],\n",
    "        include_top=False,\n",
    "        classes=CONFIG[\"num_classes\"],\n",
    "    )\n",
    "    base_model.trainable = False\n",
    "\n",
    "    inputs = tf.keras.Input(shape=CONFIG[\"input_shape\"])\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dropout(rate=CONFIG[\"dropout_rate\"], seed=CONFIG[\"seed_value\"])(\n",
    "        x\n",
    "    )\n",
    "    outputs = tf.keras.layers.Dense(units=CONFIG[\"num_classes\"], activation=\"softmax\")(\n",
    "        x\n",
    "    )\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=CONFIG[\"learning_rate\"]),\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "        metrics=[\n",
    "            tf.keras.metrics.Precision(name=\"precision\"),\n",
    "            tf.keras.metrics.Recall(name=\"recall\"),\n",
    "            tfa.metrics.F1Score(\n",
    "                num_classes=CONFIG[\"num_classes\"],\n",
    "                average=\"weighted\",\n",
    "                name=\"f1_score\",\n",
    "                threshold=0.5,\n",
    "            ),\n",
    "            tf.keras.metrics.FalseNegatives(name=\"false_negatives\"),\n",
    "            tf.keras.metrics.TruePositives(name=\"true_positives\"),\n",
    "            tf.keras.metrics.FalsePositives(name=\"false_positives\"),\n",
    "            tf.keras.metrics.TrueNegatives(name=\"true_negatives\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "checkpoint_filepath = \"checkpoints_efficientnetv2b0\"\n",
    "latest = tf.train.latest_checkpoint(checkpoint_filepath)\n",
    "\n",
    "model = create_model()\n",
    "model.load_weights(latest).expect_partial()\n",
    "\n",
    "train_images = []\n",
    "train_labels = []\n",
    "\n",
    "valid_images = []\n",
    "valid_labels = []\n",
    "\n",
    "test_images = []\n",
    "test_labels = []\n",
    "\n",
    "for X, y in train_dataset.as_numpy_iterator():\n",
    "    train_images.append(X)\n",
    "    train_labels.append(y)\n",
    "\n",
    "train_images = np.concatenate(train_images)\n",
    "train_labels = np.concatenate(train_labels)\n",
    "train_labels = np.argmax(train_labels, axis=1)\n",
    "\n",
    "for X, y in valid_dataset.as_numpy_iterator():\n",
    "    valid_images.append(X)\n",
    "    valid_labels.append(y)\n",
    "\n",
    "valid_images = np.concatenate(valid_images)\n",
    "valid_labels = np.concatenate(valid_labels)\n",
    "valid_labels = np.argmax(valid_labels, axis=1)\n",
    "\n",
    "for X, y in test_dataset.as_numpy_iterator():\n",
    "    test_images.append(X)\n",
    "    test_labels.append(y)\n",
    "\n",
    "test_images = np.concatenate(test_images)\n",
    "test_labels = np.concatenate(test_labels)\n",
    "test_labels = np.argmax(test_labels, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export results as csv\n",
    "# df = pd.DataFrame(\n",
    "#     np.array(results).reshape(1, -1),\n",
    "#     columns=[\n",
    "#         \"loss\",\n",
    "#         \"precision\",\n",
    "#         \"recall\",\n",
    "#         \"f1_score\",\n",
    "#         \"false_negatives\",\n",
    "#         \"true_positives\",\n",
    "#         \"false_positives\",\n",
    "#         \"true_negatives\",\n",
    "#     ],\n",
    "# )\n",
    "# df.to_csv(\"results/efficientnet-test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 11s 55ms/step\n",
      "2/2 [==============================] - 1s 469ms/step\n",
      "1/1 [==============================] - 0s 429ms/step\n"
     ]
    }
   ],
   "source": [
    "train_preds = model.predict(train_images)[:, 1].tolist()\n",
    "valid_preds = model.predict(valid_images)[:, 1].tolist()\n",
    "test_preds = model.predict(test_images)[:, 1].tolist()\n",
    "\n",
    "train_preds_df = pd.DataFrame(\n",
    "    columns=[\"actual\", \"predicted\"],\n",
    ")\n",
    "\n",
    "train_preds_df[\"actual\"] = train_labels\n",
    "train_preds_df[\"predicted\"] = train_preds\n",
    "\n",
    "valid_preds_df = pd.DataFrame(\n",
    "    columns=[\"actual\", \"predicted\"],\n",
    ")\n",
    "\n",
    "valid_preds_df[\"actual\"] = valid_labels\n",
    "valid_preds_df[\"predicted\"] = valid_preds\n",
    "\n",
    "test_preds_df = pd.DataFrame(\n",
    "    columns=[\"actual\", \"predicted\"],\n",
    ")\n",
    "\n",
    "test_preds_df[\"actual\"] = test_labels\n",
    "test_preds_df[\"predicted\"] = test_preds\n",
    "\n",
    "# save in float format\n",
    "train_preds_df.to_csv(\n",
    "    \"predictions/efficientnet-train-preds.csv\", index=False, float_format=\"%.3f\"\n",
    ")\n",
    "valid_preds_df.to_csv(\n",
    "    \"predictions/efficientnet-valid-preds.csv\", index=False, float_format=\"%.3f\"\n",
    ")\n",
    "test_preds_df.to_csv(\n",
    "    \"predictions/efficientnet-test-preds.csv\", index=False, float_format=\"%.3f\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
